## 4. Add new predictor: Weight from carClass, reference to FEG2017
# !!! We will deal with the order later since category: "Other"
# data.frame -> data.frame
# Add new predictor: weightLevel from carClass
# test:
addWeight <- function(dat) {
dat$weightLevel <- factor(sapply(dat$carClass, class2weight))
dat
}
# Char -> Char
# Assign weight levels according to CarLineClass
class2weight <- function(c) {
if (c == "2Seaters") {
return ("Lightest")
} else if (c == "MinicompactCars") {
return ("Sedan-1")
} else if (c == "SubcompactCars") {
return ("Sedan-2")
} else if (c == "CompactCars") {
return ("Sedan-3")
} else if (c == "MidsizeCars") {
return ("Sedan-4")
} else if (c == "LargeCars") {
return ("Sedan-5")
} else if (c == "SmallStationWagons") {
return ("Wagon-S")
} else if (c == "SmallPickupTrucks") {
return ("Truck-6k")
} else if (c == "StandardPickupTrucks" ||
c == "VansCargoTypes" ||
c == "SpecialPurposeVehicleminivan" ||
c == "SpecialPurposeVehicleSUV") {
return ("Truck-8k")
} else if (c == "VansPassengerType") {
return ("Truck-10k")
} else if (c == "Other") {
return ("Other")
} else {
return ("!!!")
}
}
# # test:
# "Lightest"  == class2weight("2Seaters")
# "Sedan-1"   == class2weight("MinicompactCars")
# "Sedan-2"   == class2weight("SubcompactCars")
# "Sedan-3"   == class2weight("CompactCars")
# "Sedan-4"   == class2weight("MidsizeCars")
# "Sedan-5"   == class2weight("LargeCars")
# "Wagon-S"   == class2weight("SmallStationWagons")
# "Truck-6k"  == class2weight("SmallPickupTrucks")
# "Truck-8k"  == class2weight("StandardPickupTrucks")
# "Truck-8k"  == class2weight("VansCargoTypes")
# "Truck-8k"  == class2weight("SpecialPurposeVehicleminivan")
# "Truck-8k"  == class2weight("SpecialPurposeVehicleSUV")
# "Truck-10k" == class2weight("VansPassengerType")
# a List of Logicals -> Data.frame
# Drop invalid observation by 3 criterion from dat$train
rmObs <- function(lol) {
wrong <- FALSE
for (logic in logicList) wrong <- wrong | logic
return(dat$train[-which(wrong),])
}
# NULL(dat$train) -> Logicals(list)
# To remove observation from dat$train by incorrect number of wheel drive
selObsByNumDrive <- function() {
df <- dat$train
nWDfromCarClass <- gsub("[^0-9]", "", substring(df$CarlineClassDesc, 2))
nWDfromDrive <- sapply(as.character(df$Drive),
function(c) ifelse(substr(c, 1, 1) == "4", "4", "2"))
return(list((nWDfromCarClass != nWDfromDrive) & (nWDfromCarClass != "")))
}
# # test:
# sum(unlist(selObsByNumDrive()))
# NULL(dat$train) -> Logicals(list)
# To remove observation from dat$train by incorrect number of gears
selObsByNumGear <- function() {
df <- dat$train
numGearFromTrans <- as.numeric(gsub("[^0-9]", "", df$Transmission))
isAV <- grepl("AV", df$Transmission) # igore AV's num of gear later
# From NumGears
numGearFromNumGear <- df$NumGears
return(list((numGearFromTrans != numGearFromNumGear) &
(!isAV & !is.na(numGearFromTrans))))
}
# # test:
# unlist(selObsByNumGear())
# NULL(dat$train) -> Logicals(list)
# To remove observation from dat$train if ValvePerCyl < 2
selObsByValvePerCyl <- function() {
df <- dat$train
numVPC <- as.numeric(as.character(df$ValvePerCyl))
VPCstrange <- numVPC < 2 | numVPC > 4
return(list(VPCstrange))
}
# # test:
# sum(unlist(selObsByValvePerCyl()))
# data.frame, Chars -> data.frame
# Drop predictors
dropPredictor <- function(df, drops) {
df[ , - which(names(df) %in% drops)]
}
# # test:
# names(dropPredictor(dat.tmp, c("ID", "FE")))
# Data.frame -> Data.frame
# Correct ValvePerCyl
correctValvePerCyl <- function(df) {
df$ValvePerCyl <- factor(df$ValvePerCyl)
return(df)
}
# Data.frame -> Data.frame
# Log FE
logFE <- function(df) {
df$logFE <- log(df$FE)
df$FE <- NULL
df
}
#### Data cleansing steps ####
### Set 4 predictors as factor
dat <- lapply(dat, setFactor)
### Add new predictors:
## 1. Drive
dat <- lapply(dat, addDrive)
## 2. ValvePerCyl
dat <- lapply(dat, addNumValves)
## 3. carClass
dat <- lapply(dat, addCarClass)
## 4. weightLevel
dat <- lapply(dat, addWeight)
# NOTE: predictors: carClass and weightLevel is collinear
### Remove invalid observations from training set
## 1. Complete cases
dat$train <- dat$train[complete.cases(dat$train),]
## 2. Incorrect observations, contradicted by the value of predictors
logicList <- c(selObsByNumDrive(), selObsByNumGear(), selObsByValvePerCyl())
dat$train <- rmObs(logicList)
### Drop useless predictors:
dat <- lapply(dat, dropPredictor,
c("ID", "NumGears", "TransCreeperGear", "DriveDesc",
"IntakeValvePerCyl", "ExhaustValvesPerCyl",
"CarlineClassDesc"))
### Reduce the level of ValvePerCyl
dat <- lapply(dat, correctValvePerCyl)
### Log FE
dat <- lapply(dat, logFE)
log(dat$train$FE)
dat$train$logFE <- log(dat$train$FE)
dat$train$FE <- NULL
source('C:/Users/USER/Desktop/R/5330s1/datCln.R', echo=TRUE)
install.packages("e1071")
library(e1071)
#### Description: ####
# Best subset in first order
library(boot)
source("datCln.R")      # training data
source("logRMSE.R")     # cost function
# Chars -> List(Chars)
# Make every possible predictor combination
makeAllPredComb <- function(allPred) {
numPred <- length(allPred)
allPredComb <- list()   # container
for (np in seq(numPred)) {
predComb <- combn(allPred, np, simplify = FALSE)
allPredComb <- c(allPredComb, predComb)
}
return(allPredComb)
}
## Class: lmCV ####
# Numeric, Chars, Numeric -> lmCV
# Make class: constructor-lmCV
lmCV <- function(logRMSE, df, predictors, nPred) {
list(logRMSE = logRMSE,
df.res = df,
predictor = predictors,
numPred = nPred) -> info
class(info) <- "lmCV"
return(info)
}
# Chars(predictor) -> lmCV
# Return class lmCV: logRMSE, predictor, df
cvGlm <- function(preds) {
df <- dat$train[, which(names(dat$train) %in% c("FE", preds))]
glm.fit <- glm(logFE ~ ., data = df)
# main
cv.err <- cv.glm(df, glm.fit, K = 10)
# information for model selection by CV
lmCV(logRMSE = cv.err$delta[1],
df = glm.fit$df.residual,
predictors =  names(df)[names(df) != "FE"],
nPred = sum(names(df) != "FE")
) -> out
return(out)
}
# # test:
# lmCV.test <- cvGlm(c("EngDispl", "NumCyl"))
# # estimated run time: 27.46 * 2^10 * 2 = 15hr
# system.time(l1oCVlm(df))
# Safe code
safeCode <- function() {
answer <- sample(1000:9999, 1)
n <- readline(prompt = paste("Enter", answer, "to continue: "))
if (as.numeric(n) != answer) {
stop("Wrong answer! The program stop.")
}
}
# prinf function
printf <- function(...) cat(sprintf(...))
# List of Chars -> List of lmCV
# Get CV information of linear models
getCVlmResult <- function(set) {
# Safety question to avoid rewrite the previous result
safeCode()
return(lapply(set, cvGlm))
}
# lmCV -> lmCV
# Remove "predictor" from lmCV
rmPred <- function(lmcv) {
lmcv[["predictor"]] <- NULL
return(lmcv)
}
# List of lmCV -> data.frame
# Make a dataframe without information of predictor
CVresult2df <- function(locv) {
locv <- lapply(locv, rmPred)
locv <- lapply(locv, unclass)
locv <- lapply(locv, data.frame)
out <- do.call(rbind, locv)
return(out)
}
#### Data ####
# Using data: dat$train
# predictor set
allPred <- names(dat$train)[!names(dat$train) %in% "FE"]
predSet <- allPred[!allPred %in% c("weightLevel")]    # Drop
#### Description: ####
# Best subset in first order
library(boot)
source("datCln.R")      # training data
# Chars -> List(Chars)
# Make every possible predictor combination
makeAllPredComb <- function(allPred) {
numPred <- length(allPred)
allPredComb <- list()   # container
for (np in seq(numPred)) {
predComb <- combn(allPred, np, simplify = FALSE)
allPredComb <- c(allPredComb, predComb)
}
return(allPredComb)
}
## Class: lmCV ####
# Numeric, Chars, Numeric -> lmCV
# Make class: constructor-lmCV
lmCV <- function(logRMSE, df, predictors, nPred) {
list(logRMSE = logRMSE,
df.res = df,
predictor = predictors,
numPred = nPred) -> info
class(info) <- "lmCV"
return(info)
}
# Chars(predictor) -> lmCV
# Return class lmCV: logRMSE, predictor, df
cvGlm <- function(preds) {
df <- dat$train[, which(names(dat$train) %in% c("FE", preds))]
glm.fit <- glm(logFE ~ ., data = df)
# main
cv.err <- cv.glm(df, glm.fit, K = 10)
# information for model selection by CV
lmCV(logRMSE = cv.err$delta[1],
df = glm.fit$df.residual,
predictors =  names(df)[names(df) != "FE"],
nPred = sum(names(df) != "FE")
) -> out
return(out)
}
# # test:
# lmCV.test <- cvGlm(c("EngDispl", "NumCyl"))
# # estimated run time: 27.46 * 2^10 * 2 = 15hr
# system.time(l1oCVlm(df))
# Safe code
safeCode <- function() {
answer <- sample(1000:9999, 1)
n <- readline(prompt = paste("Enter", answer, "to continue: "))
if (as.numeric(n) != answer) {
stop("Wrong answer! The program stop.")
}
}
# prinf function
printf <- function(...) cat(sprintf(...))
# List of Chars -> List of lmCV
# Get CV information of linear models
getCVlmResult <- function(set) {
# Safety question to avoid rewrite the previous result
safeCode()
return(lapply(set, cvGlm))
}
# lmCV -> lmCV
# Remove "predictor" from lmCV
rmPred <- function(lmcv) {
lmcv[["predictor"]] <- NULL
return(lmcv)
}
# List of lmCV -> data.frame
# Make a dataframe without information of predictor
CVresult2df <- function(locv) {
locv <- lapply(locv, rmPred)
locv <- lapply(locv, unclass)
locv <- lapply(locv, data.frame)
out <- do.call(rbind, locv)
return(out)
}
#### Data ####
# Using data: dat$train
# predictor set
allPred <- names(dat$train)[!names(dat$train) %in% "FE"]
predSet <- allPred[!allPred %in% c("weightLevel")]    # Drop
allPred
allPred <- names(dat$train)[!names(dat$train) %in% "logFE"]
PRED_SET <- allPred[!allPred %in% c("weightLevel")]    # Drop
rm(predSet)
load("C:/Users/USER/Desktop/R/5330s1/f10cvlm_2.RData")
lmCV.df <- CVresult2df(RESULT)
# The best subset predictor:
bestIndex <- which.min(sapply(RESULT, function(lmCV) lmCV[["logRMSE"]]))
RESULT[[bestIndex]]
RESULT <- f10cvlm.2
lmCV.df <- CVresult2df(RESULT)
bestIndex <- which.min(sapply(RESULT, function(lmCV) lmCV[["logRMSE"]]))
RESULT[[bestIndex]]
rep(PRED_SET, 100)
SELECT_PRED <- RESULT[[bestIndex]]$predictor
rep(SELECT_PRED, list = TRUE)
rep(SELECT_PRED, 2, list = TRUE)
rep(list(SELECT_PRED), 2, list = TRUE)
tmp <- rep(list(SELECT_PRED), 100)
tmp <- cvGlm(tmp)
tmp <- lapply(tmp, cvGlm)
tmp <- lapply(tmp, cvGlm)
cvGlm <- function(preds) {
df <- dat$train[, which(names(dat$train) %in% c("logFE", preds))]
glm.fit <- glm(logFE ~ ., data = df)
# main
cv.err <- cv.glm(df, glm.fit, K = 10)
# information for model selection by CV
lmCV(logRMSE = cv.err$delta[1],
df = glm.fit$df.residual,
predictors =  names(df)[names(df) != "logFE"],
nPred = sum(names(df) != "logFE")
) -> out
return(out)
}
tmp <- lapply(tmp, cvGlm)
sapply(tmp, function(l) l[["logRMSE"]])
cvGlm <- function(preds) {
dat <- dat$train[, which(names(dat$train) %in% c("logFE", preds))]
glm.fit <- glm(logFE ~ ., data = df)
# main
cv.err <- cv.glm(dat, glm.fit, K = 10)
# information for model selection by CV
lmCV(logRMSE = cv.err$delta[1],
df = glm.fit$df.residual,
predictors =  names(df)[names(df) != "logFE"],
nPred = sum(names(df) != "logFE")
) -> out
return(out)
}
cvGlm <- function(preds) {
# data in use
dat <- dat$train[, which(names(dat$train) %in% c("logFE", preds))]
# main
glm.fit <- glm(logFE ~ ., data = dat)
cv.err <- cv.glm(dat, glm.fit, K = 10)
# information for model selection by CV
lmCV(logRMSE = cv.err$delta[1],
df = glm.fit$df.residual,
predictors =  names(dat)[names(dat) != "logFE"],
nPred = sum(names(dat) != "logFE")
) -> out
return(out)
}
tmp <- sapply(tmp, function(l) l[["logRMSE"]])
boxplot(tmp)
mean(tmp)
names(dat$test)
names(dat$test) == SELECT_PRED
names(dat$test) %in% SELECT_PRED
str(dat$test)
which(names(dat$test) %in% SELECT_PRED)
dat$test[which(names(dat$test) %in% SELECT_PRED)]
tmp <- dat$test[which(names(dat$test) %in% SELECT_PRED)]
str(tmp)
tmp <- dat$train[which(names(dat$train) %in% c("logFE", SELECT_PRED))]
str(tmp)
SELECT_LM <- lm(logFE ~ ., tmp)
tmp <- dat$test[which(names(dat$test) %in% SELECT_PRED)]
predict(SELECT_LM, tmp)
exp(predict(SELECT_LM, tmp))
plot(exp(predict(SELECT_LM, tmp)))
PREDICTION <- exp(predict(SELECT_LM, tmp))
plot(sort(PREDICTION))
tmp <- rep(list(SELECT_PRED), 100)
tmp <- lapply(tmp, cvGlm)
TEST_ERROR <- lapply(tmp, cvGlm)
0.0923^2
0.00723^(1/2)
cvGlm <- function(preds) {
# data in use
dat <- dat$train[, which(names(dat$train) %in% c("logFE", preds))]
# main
glm.fit <- glm(logFE ~ ., data = dat)
cv.err <- cv.glm(dat, glm.fit, K = 10)
# information for model selection by CV
lmCV(logRMSE = sqrt(cv.err$delta[1]),
df = glm.fit$df.residual,
predictors =  names(dat)[names(dat) != "logFE"],
nPred = sum(names(dat) != "logFE")
) -> out
return(out)
}
TEST_ERROR <- lapply(tmp, cvGlm)
tmp <- sapply(TEST_ERROR, function(l) l[["logRMSE"]])
boxplot(tmp)
mean(tmp)
load("C:/Users/USER/Desktop/R/5330s1/CVresults_1.RData")
RESULT <- CVresults.1
lmCV.df <- CVresult2df(RESULT)
bestIndex <- which.min(sapply(RESULT, function(lmCV) lmCV[["logRMSE"]]))
RESULT[[bestIndex]]
SELECT_PRED <- RESULT[[bestIndex]]$predictor
tmp <- rep(list(SELECT_PRED), 100)
TEST_ERROR <- lapply(tmp, cvGlm)
library(boot)
#### Description: ####
# Best subset in first order
library(boot)
source("datCln.R")      # training data
# Chars -> List(Chars)
# Make every possible predictor combination
makeAllPredComb <- function(allPred) {
numPred <- length(allPred)
allPredComb <- list()   # container
for (np in seq(numPred)) {
predComb <- combn(allPred, np, simplify = FALSE)
allPredComb <- c(allPredComb, predComb)
}
return(allPredComb)
}
## Class: lmCV ####
# Numeric, Chars, Numeric -> lmCV
# Make class: constructor-lmCV
lmCV <- function(logRMSE, df, predictors, nPred) {
list(logRMSE = logRMSE,
df.res = df,
predictor = predictors,
numPred = nPred) -> info
class(info) <- "lmCV"
return(info)
}
# Chars(predictor) -> lmCV
# Return class lmCV: logRMSE, predictor, df
cvGlm <- function(preds) {
# data in use
dat <- dat$train[, which(names(dat$train) %in% c("logFE", preds))]
# main
glm.fit <- glm(logFE ~ ., data = dat)
cv.err <- cv.glm(dat, glm.fit, K = 10)
# information for model selection by CV
lmCV(logRMSE = sqrt(cv.err$delta[1]),
df = glm.fit$df.residual,
predictors =  names(dat)[names(dat) != "logFE"],
nPred = sum(names(dat) != "logFE")
) -> out
return(out)
}
# # test:
# lmCV.test <- cvGlm(c("EngDispl", "NumCyl"))
# Safe code
safeCode <- function() {
answer <- sample(1000:9999, 1)
n <- readline(prompt = paste("Enter", answer, "to continue: "))
if (as.numeric(n) != answer) {
stop("Wrong answer! The program stop.")
}
}
# prinf function
printf <- function(...) cat(sprintf(...))
# List of Chars -> List of lmCV
# Get CV information of linear models
getCVlmResult <- function(set) {
# Safety question to avoid rewrite the previous result
safeCode()
return(lapply(set, cvGlm))
}
# lmCV -> lmCV
# Remove "predictor" from lmCV
rmPred <- function(lmcv) {
lmcv[["predictor"]] <- NULL
return(lmcv)
}
# List of lmCV -> data.frame
# Make a dataframe without information of predictor
CVresult2df <- function(locv) {
locv <- lapply(locv, rmPred)
locv <- lapply(locv, unclass)
locv <- lapply(locv, data.frame)
out <- do.call(rbind, locv)
return(out)
}
tmp <- rep(list(SELECT_PRED), 100)
TEST_ERROR <- lapply(tmp, cvGlm)
tmp <- sapply(TEST_ERROR, function(l) l[["logRMSE"]])
boxplot(tmp)
mean(tmp)
